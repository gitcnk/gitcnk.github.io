[
["index.html", "Statistics: A Critical Look Preface", " Statistics: A Critical Look CK 2/10/2019 Preface This book grew out of my class notes in Stat-2 and my observations of other instructors around the world. I claim no originality in my thoughts or examples. Almost all of them are influenced, shaped, and refined by listening to other people. I thank ALL of them for their inspiring lectures and insightful examples which made this work possible. "],
["some-preliminary-questions.html", "1 Some Preliminary Questions", " 1 Some Preliminary Questions In your introductory statistics courses you have learned several fundemental ideas in statistics. It is now time to revisit them and see how well you have understood them. The following questions will help us do it. Let’s dive in. What is statistical inference? What is a parameter? What is a statistic? What is ‘good’ sample? What is a sampling distribution? What is a confidence interval? How do you interpret a 95% confidence interval? What is the true meaning of the 95%? Hint: think about the idea of the capture rate What is the Central Limit Theorem? What is the role of the Central Limit Theorem in the construction of a confidence interval? "],
["sampling-distributions.html", "2 Sampling Distributions 2.1 How to Construct a Sampling Distribution? 2.2 Some Questions to Ponder", " 2 Sampling Distributions Many statistical problems require us to estimate population parameters or model parameters. For example, we might want to know the percentage of binge drinkers in a college campus. A good point estimate for this parameter is the sample proportion \\(\\hat p\\). However, this value does not mean much unless we provide the variability of it. That is, we’d like to know if we take a another sample of the same size (from the same population), how different of a \\(\\hat p\\) would we see compared to the one we had before. In other words, how much variance can we expect in \\(\\hat p\\) from sample to sample. Let’s look at an example. Example 2.1 Consider the following simulated population. Although this is a simulated population it is not that uncommon. For example, most service time distributions, like time to complete a transaction, time to repair a car, etc follows this pattern. Suppose we want to estimate the mean of this population using only a random sample of say, \\(n=30\\). The following code will do just that. mysample &lt;- sample(mypop, size = 30) mean(mysample) ## [1] 18.79358 What if we take another sample and calculate the mean. It will be different from the one above. mysample &lt;- sample(mypop, size = 30) mean(mysample) ## [1] 19.73084 This variability in our sample statistics is what we are going to study in this chapter. In particular, we are going to look at the variability of two sample statistics: The sample mean (\\(\\bar x\\)) The sample maximum. 2.1 How to Construct a Sampling Distribution? Take a random sample from this population and calculate (and store) the statistic we want. Then take another random sample of the same size as above and calculate (and store) the statistic we want. Repeat this process a large number of times. This will give us a collection of values for the statistic. For example, if we studying the sample mean \\(\\bar x\\) we will have a bunch of sample means corresponding to each of the random samples. Plot these sample statistics in a histogram. This will give us a graphical representation of the variability of the statistic. The following dataset contains a bunch (10000 to be exact) of random samples of size \\(n=30\\) from the above service time population along with some sample statistics calculated for each random sample. service_time &lt;- read_csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/Service_Time_n30.csv&#39;) Let’s plot the distribution of the following sample statistics: The Sample Mean (\\(\\bar x\\)) The Sample Maximum. These distributions (above) are called sampling distributions. In particular, the first one is the sampling distribution of the sample mean and the second is the sampling distribution of the sample maximum. Therefore we can define a sampling distribution of a statistic as follows. Definition 2.1 Sampling Distribution of a sample statistic is defined as the distribution of values of that statistic under repeated sampling. Note that the key phrase of the above definition: ‘Sampling’. It tells us that this has something to with samples coming from a population. The next term is ‘Distribution’. It tells us how the sample statistic chages from sample to sample. Always remember to think about this twp terms carefully for few seconds before you answer any question about a sample statistic. Another important thing to remember is that the sampling distribution always depends on the size of the sample (\\(n\\)) we draw from the population. This is a crucial fact. For example, the above two sampling distributions are created from samples of size \\(n=30\\). Always mention this fact (sample size) when you describe a sampling distribution. To put this idea in perspective, let’s compare the two sampling distrubutions of \\(\\bar x\\) and sample maximum for samples of size \\(n=100\\). Do you see anything special? You’ll notice the following about the sample mean: Shape “Center” or typical value Spread n = 30 slightly skewed around 20 spans 20 units n = 100 quite normal very close to 20 spans 10 units You’ll notice the following about the sample maximum: Shape “Center” or typical value Spread n = 30 skewed around 80 spans 100 units n = 100 skewed around 100 spans 80 units Why are these numbers useful to us? They tell us whether our statistic, on average, is “close” to the population parameter. They help us to determine, how likely are we going to be very far from the target (population parameter) In the case of the sample mean we see that our estimates are quite close to the true value (\\(\\mu = 20\\)). And the likelihood that we are off too much decreases rapidly as \\(n\\) increases (from 30 to 100). In contrast, the sample maximum is biased (or underestimates) the true value (population max = around 150, technically this value is \\(\\infty\\)) and the likelihood that we are off does not decrease that much as \\(n\\) increases (from 30 to 100). These two properties - the “center” or the typical value of the statistic - the “spread” of the statistic and its relationship to sample size \\(n\\). is defined more technically as follows. Definition 2.2 Center of the sampling distribution It is defined as the expected value of the statistic. It is a measure we use to see whether our statistic, on average, hits the target (population parameter). Here is an example: Example 2.2 Consider the sample mean \\(\\bar x = \\frac{\\sum_{i=1}^n x_i}{n}\\). The expected value of \\(\\bar x\\) is \\(E(\\bar x) = E\\left[\\frac{\\sum_{i=1}^n x_i}{n}\\right] = \\mu\\). Can you think how I got this value? Definition 2.3 The standard Error(variablity of the sampling distribution) is defined as the standard deviation of the statistic. Here is an example. Example 2.3 Consider the sample mean \\(\\bar x = \\frac{\\sum_{i=1}^n x_i}{n}\\). The SE of \\(\\bar x\\) is \\(\\sigma/\\sqrt n\\), where \\(\\sigma\\) is the population standard deviation. Can you think how I got this value? Calculating the expected value and standard error in the above example is not that difficult. You will be doing this as a homework assignment for this chapter. The key is to find the variance of \\(\\bar x\\). That is, find \\(V(\\bar x)\\) first. Then take the square root of it to find the SE. All the plot and the definitions look very good, except that we have a major problem in our hand. The next section will expore those issues. 2.2 Some Questions to Ponder 1. Is it practically possible to know the shape of the sampling distribution of a statatistic? Answer - PART 1: In general, no. Why? Think about the process of constructing a sampling distribution. It require us to draw a LOT of samples from a population. Now imagine the time, energy and the costs associated with this process in real life. For example, suppose you want to know the sampling distribution of the average commute times in NYC for samples of size \\(n=30\\). You’ll have to visit the subway stations (randomly) and interview 30 people (randomly) to gather information about their commute times. Then, you need to do this again and again for about 10,000 times! This is highly impractical and time consuming. In fact, if you think about it, it is a complete waste of your time and resources. Because, you could have simply taken a large sample of size 30,000 instead of 10,000 samples of size 30! Now you might wonder how do we even attempt to find this sampling distrubution. That’s the second part of the answer. Answer - PART 2: In some special cases we can find it. How? Long time ago (a very long time ago), statisticians have figured out that the statistics like the sample mean (averages in general), have a sampling ditribution which is normally distributed under some conditions. This fact is one of the celebrated theorems in statistics. It is called the Central Limit Theorem. It says that the sample mean \\(\\bar x\\) follows a normal model with center beign at the true population mean. More precisely, \\[ \\bar x \\sim N(\\mu , \\frac{\\sigma}{\\sqrt n})\\] We will be studying this in Chapter 3. Note: There are other statistics whose sampling distributions can be found usingtheoretical tools that are beyond the scope of this class. Math 351 and Math 352 is where we study them. 2. Is it possible to calculate the center (expected value), and standard error (SE) of a sampling distribution? Answer: In some cases, yes, we can find it. For example, we already found this for \\(\\bar x\\). As for the sample maximum , we can find the expected value and SE for the with some other tools that are beyond the scoope of this class. In some cases, it is imposible to do this. This leads us to the final question. 3. Is there a reasonable way to get some sense about the shape, center, and spread of the sampling distribution of a statistics? Answer: Yes. We appeal to a more modern method called the bootstrap method. We are going to learn this in Chapter 4. "],
["the-infamous-n-30-rule.html", "3 The Infamous ‘n &gt; 30 rule’ 3.1 Central Limit Theorem 3.2 Explorations with Real Data 3.3 Some Questions to Ponder", " 3 The Infamous ‘n &gt; 30 rule’ The title of this chapter may be a bit confusing. The formal title should be “The Central Limit Theorem”. But I wanted to make a point in this chapter about the misuses of this remarkable theorem. Let’s dive in. 3.1 Central Limit Theorem Before we formally introduce this theorem, let us first look at a motivating example. Recall the service time example we saw in Chapter 2. In that, we saw that the sampling distribution of \\(\\bar x\\) tend to follow a normal model as \\(n\\) increases while the sampling distribution of the sample maximum does not show any normal behavior. To put things in perspective, let us look at some plots. Sampling Distributions of the Sample Mean Carefully examine the plots above. In particular, pay attention to the following: shape spread (look at the scale of the x-axis) The behavior of the sample mean as \\(n\\) increases has two noteworthy aspects. For larger samples, like \\(n &gt; 100\\), the sampling distribution of \\(\\bar x\\) is normal with the “center” being very close to the true mean of the population that we sample from. The spread get’s smaller as \\(n\\) increases. Why is this important to us? First, the normal distribution is something that is easy to understand and we know how to calculate probabilities using the normal model. Also, recall the “68-95-99.7% rule” which describes how the probabilities change with respect to the standard deviation (spread) of the model. So, anything that follows a normal model is good news for the statistician, because we know a LOT about this model. Second, the spread (standard deviation) decreases as \\(n\\) increases. This is encouraging because it ensures that for larger samples we are not too far off from the “center” of the distribution which happens to be very close to the true mean. In fact, These facts were discovered a long time ago and they are summarized in one of the celebrated theorems in statistics. It is called the Central Limit Theorem. It says, under some conditions, the sample mean \\(\\bar x\\) follows a normal model with center being at the true population mean and the spread decreases at a rate of \\(1/\\sqrt n\\). We can denote this more succinctly as follows: \\[ \\bar x \\sim N(\\mu , \\frac{\\sigma}{\\sqrt n})\\] We will explore this theorem further in this chapter. But, let us first look at an example ( a statistic) that do not agree with the Central Limit Theorem. This example helps us to understand the core idea of this theorem. The plots below are sampling distributions of the sample maximum constructed from the same service time population (Example 2.1). Carefully look at the shape and spread of these plots. Sampling Distributions of the Sample Maximum You’ll notice immediately that the shape does not look normal even for \\(n=300\\). Also, the spread does not decrease that much. In fact, the spread is fairly constant across all 4 distributions. This is NOT an accident. The reason that this statistic, the sample maximum, does not obey the Central Limit Theorem is because it is NOT an average constructed from the sample. This is the core idea of this theorem. The normal behavior of the sampling distribution is ONLY applicable to sample averages. Here is another example to demonstrate this. Suppose we want to know the percentage of binge drinkers in college campuses. A good point estimate for this parameter is the sample proportion \\(\\hat p\\). What is the sampling distribution of this statistic \\(\\hat p\\)? Let’s create a small simulation to find this out. Here are the steps: Create a population of binge and non-binge drinkers Draw a sample from this population, say of size \\(n=30\\) and calculate \\(\\hat p\\). Repeat the above step (sampling and calculation of \\(\\hat p\\)) for a large number of times and plot the distribution of those \\(\\hat p\\) values. population_size &lt;- 1E6 sample_size &lt;- 20 my_drinking_pop &lt;- rbinom(population_size, 1, prob = 0.20) simulated_samples &lt;- 1E3 phat &lt;- 0 #storage bucket for(i in 1:simulated_samples) { mysample &lt;- sample(my_drinking_pop, size = sample_size) phat[i] &lt;- sum(mysample)/sample_size } ggplot() + geom_histogram(mapping = aes(x = phat), bins = 13) + labs( title = &#39;Sampling Distribution of phat&#39;, subtitle = &#39;Sample size n = 20&#39;, x = &#39;phat&#39;) As you can see, for \\(n=20\\) the sampling distribution of \\(\\hat p\\) is somewhat skewed. We can increase the sample size and see what happens to the sampling distribution. The following plots shows relationship of \\(n\\) with the shape and spread of the sampling distribution. It seems like the Central Limit Theorem is at play. That is, the sampling distribution of \\(\\hat p\\) looks normal for large \\(n\\). But, we know that the theorem only applies to AVERAGES. Now you might wonder is \\(\\hat p\\) an average? The answer is ‘Yes’. It is a proper average. It does not look like one, but we can show why it is an average. Let’s denote a random sample of binge grinkers as \\(x_1, x_2, \\ldots, x_n\\). Each \\(x_i\\) is either a \\(1\\) or \\(0\\), depending on whether the person is a binge drinker or not. Now if we write out the formula for the sample proportion \\(\\hat p\\) you’ll see why it is an average. \\[ \\begin{array}{ll} \\hat p &amp;= \\frac{Number \\ of \\ binge \\ drinkers \\ in\\ the\\ sample}{Total\\ number\\ of\\ people\\ in\\ the\\ sample} \\\\ &amp;= \\frac{\\sum_{i=1}^nx_i}{n} \\end{array} \\] As shown, above, \\(\\hat p\\) is an average and that’s why the sampling distribution of \\(\\hat p\\) behaves according to the Central Limit Theorem (CLT). It is now time to take stock of the important facts that we observed so far. Consider the following summary table: Statistic Obeys CLT Shape at n = 30 Shape at n = 50 Sample Mean (\\(\\bar x\\)) Yes Skewed Skewed Sample Max No not relevant not relevant Sample proportion (\\(\\hat p\\)) Yes Skewed Skewed Now you probably see why I labeled this chapter as “The infamous n &gt; 30 rule”. Most people believe that we can make use of the CLT if the sample size is “larger than 30”. But, as you saw in the above examples, this “rule” is extremely questionable. You might object to this observation by saying: “Well, you used simulated data. You could have cherry-picked your data to”prove&quot; a point&quot;. Certainly, this is a valid (and reasonable) objection. Let us therefore look into some real datasets. 3.2 Explorations with Real Data In this section, we will look at 3 examples with real data. Keep in mind, these examples are hard to find. Because, it is very unlikely that we have ALL the members (data) available from a population. If we have have all data points from a population, there is no need for statistical inference. These examples are chosen to highlight one of the main misconceptions of CLT, namely, the ‘\\(n&gt;30\\) rule’ (the title of this chapter!). In each example, when you look at the sampling distributions of the mean, pay close attention to the sample size. Ask yourself, at what sample size does the sampling distributions start to look more like a normal model. NBA Player Salaries Example 3.1 NBA player salaries in 2016 season (Kolby Bryant’s last season). As with any other salary distribution, these values are skewed with few players earning a LOT. The following plots show the population distribution of NBA player salaries in 2016. The following plots show the sampling distributions are made from samples from this population. Observations The first thing you will notice that at \\(n=30\\), the skewness is quite visible. At \\(n=50\\), it is still slightly skewed. So may be we ought to modify the “rule” as \\(n&gt;50\\)? Undergraduates in US Colleges and Universities The second example is from ALL US colleges and universities. Note that there are about 5000 colleges and universities in the US. However, there are many missing datapoints since it is hard to find all data from all institutions. As a result the population size is 1269. In this example, we are interested in the undergraduate population in US colleges and universities. Here is the population distribution. The following plots show the sampling distributions are made from random samples from the above population. Even this example, at \\(n=30\\) the skewness is quite visible. At \\(n=50\\) the skewness is still there. Even at \\(n=100\\), if you look at carefully you’ll see that there is a slight skewness. So may be the rule should be \\(n&gt;100\\)? Departure Delays in LaGuardia Airport Now, let’s look at the final example. This data is about flight delays. You can find a lot of information and download data for past years from this website: https://www.transtats.bts.gov/ONTIME/Departures.aspx The dataset we are looking at consists of ALL flights that departed from LaGuardia airport in 2017 from the three main carriers (American Airlines, Delta, and United). That is, all departures from Jan 1 2017 to Dec 31 2017. The variable of interest is departure delay time. First, let’s take a look at the population distribution of departure delays. Now let’s look at the corresponding sampling distribution for the sample mean. You can clearly see that none of the above plots look normally distributed. Let us increase the sample size even further and see at what point it starts to look normal. The above plots underscore the main point of this chapter. The common belief that if \\(n&gt;30\\) the CLT is applicable for sample means. This is a very misguided notion. The above plots demonstrate that for this dataset the sample size \\(n\\) should be around 1000(!) to see a symmetric, bell shaped sampling distribution. By now, hopefully, you are convinced that there is a problem in the common understanding of the Central Limit Theorem. The main point in this chapter is to be aware of this issue and be very cautious in using the CLT. Run the following command in RStudio to open the data and do this exploration yourself. It will open an app that allows you to change the sample size and construct sampling distributions on your own. library(shiny) runGitHub(repo = &quot;gitcnk/Apps/&quot;, subdir=&#39;CLT_NBA&#39;) library(shiny) runGitHub(repo = &quot;gitcnk/Apps/&quot;, subdir=&#39;CLT_Colleges&#39;) 3.3 Some Questions to Ponder 1. Why do people advocate for the &quot;n &gt; 30 rule&quot; if it is problematic? Answer: Hard to say. My guess is that this is similar to our beliefs and practices of recycling plastics. That is, we were told to toss our plastics into the recycling bin and we normally believe that those plastics will get recycled somehow. But the reality is MUCH more complicated than that. Here is a wonderful documentary about recycling pastics and its realities: https://www.youtube.com/watch?v=-dk3NOEgX7o Similarly, the Central Limit Theorem should be used with caution. There are many factors that we need to look into before we jump in and use this theorem. The issue mainly lies in understanding the population and its distribution. Since we don’t have access to the population (if we did we’ll be all be at the beach!), we have to rely on a random sample to make a judgement about the variability, skewness and outlier in the population. This is a very challenging task. For example, recall the flight delay example. If we haven’t had the entire dataset with us, would we have guessed that the population may look extremely skewed? Would we have guessed that there might be flights that are delayed 20 hours! Consider an example about cancer medication and their survival time. Unless we have an in depth knowledge about the cancer and the drug, it would be very difficult to even have a vague idea about the population distribution. In this case we need to rely on domain experts to tell us more about variance, skewness and extreme values. In reality we only have a single sample from a given population and we need to make a judgement call on whether CLT is an appropriate technique to use with this sample. That’s why it is better to have a healthy level of skepticism about our data before we proceed. As good statisticians, it is our duty to inform our clients about the limitations of the data and the inferences that we draw from them. 2. What are some of the signs that CLT may be questionble? Answer: If you look back the simulated data examples and real data examples you’ll see that, for skewed data, it takes much larger sample size (in some cases in the 100’s) to use the CLT. Also, in the final example with departure delays, the population was not only highly skewed but also contained extremely large values in the tail. This is definitely a red flag. So if you believe that the population is skewed and/or with extreme values you need to be super careful with the CLT. 3. How can we know whether the population is skewed or not when we don&#39;t have access to the entire population? Answer: This one is tricky. Yes, we never have access to the entire population. All we have is a single random sample. If our sampling process had done a good job in capturing the variance in the population then it will provide important clues about the skewness and presence of extreme values in the population. For example, here are two random sample of sizes \\(n=30\\) and \\(n=100\\) respectively from the flight delay example. As you can see from the above two plots, the bigger sample was able able to capture more of the variance in the population but still failed to include some of the really large delay times even with \\(n=100\\). But, this is where the statisticians need to step in. We can raise a red flag against anyone who is tempted to use the CLT with a sample like this for hypothesis testing or confidence intervals. We can educate them to see the danger of using CLT with a sample like this. 4. If we see extreme values (like in the above two samples) should we remove them and proceed to use the CLT? Answer: The safe answer is ‘no’. Removing data points has to be done with extreme care. There are some instances where removing extreme values may be legitimate. If the data values are recorded incorrectly (errors in the data). For example, someone might have keyed in 10,000 (incorrect) instead of 1000 (correct). If we know for sure that this is the case, first we should try to find the correct value, if not remove it. If the purpose of the analysis dictates the removal of extreme values. Can you think of a situation where it might be essential to remove extreme values? This will be a part of your HW for this chapter. "],
["the-bootstrap-method.html", "4 The Bootstrap Method 4.1 Bootstrap Sampling Distribution 4.2 Bootstrap Confidence Intervals 4.3 Things to Ponder", " 4 The Bootstrap Method In short, the Bootstrap is a technique that we can use to get an idea about the sampling distribution of a statistic. It helps us to calculate the variability (standard error) of that statistic and also give us important clues about the shape of its TRUE sampling distribution. In addition, the Bootstrap method is a easy to use alternative to construct confidence intervals for population parameters especially when classical theory (which based on the Central Limit Theorem) is questionable or does not apply. In summary, it is another (useful) tool in our toolbox. But, it is NOT necessarily a `better’ tool. It depends on what we mean by ‘better’. In this chapter we will explore the pros and cons of this new method. Let’s dive it. 4.1 Bootstrap Sampling Distribution Example 4.1 Recall the service time distribution in Example 2.1. Service times to complete a task like repir a car, restaurent take-out order, etc. typically have a distribution of this type. What we normally see is a sample from the above population which may look like this: The sample mean is: mean(mysample) ## [1] 22.04726 But, we’d like to know how accurate this estimate is. That is, we would like to know the standard error. In this case, the sample statistic is quite simple. It is the sample average \\(\\bar x\\). We know how to find the standard error of this statistics using the \\(Var(\\cdot)\\) operator (Refer to you HW in Chapter 2). Here is the answer: sd(mysample)/sqrt(n) ## [1] 4.016029 We also learned in Chapter 2 that the standard error is nothing but the spread (std. dev) of the sampling distribution of \\(\\bar x\\). Is there a way to find the sampling distribution of \\(\\bar x\\)? Unfortunately, as we discussed in Chapter 2, it is not possible to find the sampling distribution of a statistic. This is where the bootstrap method comes in handy. Here is how we use it. What we have: A single random sample from the population. In this case a sample of size \\(n=30\\). What we assume: This sample is a good enough representation of the population. In other word, the shape and spread of this sample is similar to the population. We then proceed as if this sample is our “population”. The rest of the steps is almost identical to what we had in Chapter 2, when we build the sampling distribution of \\(\\bar x\\). Take a random sample of size \\(n\\) (30 in this case) from this sample with replacement and calculate (and store) the statistic we want. Doing the sampling with replacement is the crucial point here. If we did not replace the data points, we will end up with the sample. For example, if our sample is (1,2,3), a resample with replacement looks like this: (2,2,3). Another resample might be (1,1,1). Since the sampling is done with replacement, we call these new samples bootstrap samples. Then take another bootstrap random sample of the same size as above and calculate (and store) the statistic we want. Repeat this process a large number of times. This will give us a collection of values for the statistic. We call these ‘bootstrap sample statistics’. For example, if we studying the sample mean \\(\\bar x\\) we will have a bunch of bootstrap sample means corresponding to each of the bootstrap samples. Plot these bootstrap sample statistics in a histogram. This will give us a graphical representation of the variance of the statistic. The following plot shows the bootstrap sampling distribution of \\(\\bar x\\) constructed out of the above random sample. One of the main reasons that we use the Bootstrap method is to get an idea of the uncertainty of our sample mean (\\(\\bar x\\)). In our case, the sample mean was 22.01. But we wanted to find the uncertainty of this estimate which is nothing but the spread of the true sampling distribution of \\(\\bar x\\). Since the bootstrap sampling distribution is an approximation of the true sampling distribution, we can calculate the spread of the bootstrap sampling distribution to get an estimate of the true standard error. As the table below shows, this method is extremely accurate. True SE Bootstrap SE 4.01 3.95 Copy and paste the following R commands in R Studio to open an app to experiment with this data so that you can convince yourself that the Bootstrap SE is a reasonable estimate of the true SE. library(shiny) runGitHub(repo = &quot;gitcnk/Apps/&quot;, subdir=&#39;BootstrapSE&#39;) After experimenting with the app you must be (hopefully) convinced that the Bootstrap method provides an alternative to find the standard error of a statistic. But, you might wonder why do we need an alternative when we already know how to find it theoretically. The reason is, sometimes (in fact in many real life cases), the statistic we are interested in is a more complicated one. Consider the following example: Example 4.2 Suppose you are interested in the correlation between amount of sleep and GPA. The sample correlation coefficient (\\(r\\)) for this data is 0.88. As before, we would like to know the standard error (SE) of this statistic. The sample correlation coefficient is defined as \\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y) }{sd_x sd_y}\\] It is virtually impossible to find the variance (\\(Var[ \\ r\\ ]\\)) of the above expression. Note that ratios of random variables are notoriously difficult to handle. But we can use the Bootstrap method to find an estimate of the SE using the Bootstrap sampling distribution of the correlation coefficient, \\(r\\). The following plot shows the Bootstrap sampling distribution of \\(r\\) from the above dataset. Based on this analysis we can say the following about the sleep v. GPA data: True correlation Sample correlation (\\(r\\)) True Standard Error Bootstrap Standard Error We’ll never know 0.88 Very Difficult to find 0.035 One thing to stress here is that, we would never know the true correlation or its variance. However, the above table provides a better understanding of what it might be. It tells us that the sample correlation is 0.88 but the uncertainly is on average, about 0.035 units. Hopefully, this example will convince you to believe the utility of the Bootstrap method as technique to find the standard error of any statistic. Simply create the bootstrap sampling distribution and find its spread (standard deviation). To drive this point, let us look at another example. Example 4.3 This was a study on in-hospital deaths from myocardial infarction with ST elevation (STEMI) which was done in Oregon. Summary stats of the study is as follows: 9 hospitals in the Providence Health system in Oregon. Study period : from 2002 to 2003. 913 STEMI patients treated, of whom 105 died (in-hospital deaths) from myocardial infarction with ST elevation (STEMI). Each patient was assigned a Thrombolysis in Myocardial Infarction Risk Scores The following table contains data grouped by risk score. In addition, information about the national mortality rate for each risk category is also given. Risk_score Patients Deaths NRMI_Mortality_in_percents 0 34 0 0.17 1 75 0 0.67 2 88 1 1.70 3 73 1 2.88 4 91 4 5.32 5 110 11 9.43 6 94 19 14.12 7 70 19 18.86 8 51 8 19.93 9 47 12 24.30 10 31 6 26.22 11–14 21 7 33.42 No score 128 17 14.42 Overall Question: How “good” is this hospital system (in terms of death rates) compared to the national level? How could we begin to answer this question? You might suggest to look at the expected deaths IF the mortality rate for the hospital system is the same as the national rate. For example, for risk level 5, the expected number of deaths is \\(110*9.43\\% \\ = 10.37\\), which is not that different from the actual death count of 11. If we do this for all risk levels we get a column of expected deaths. From that we can look at the difference between the observed and the expected deaths. But, there is a problem in this approach. Consider the following two cases: Case 1 Case 2 Observed = 10 Observed = 100 Expected = 5 Expected = 95 In both cases, the difference is 5, but we can all agree that a deviation of 5 is more severe in Case 1 than it is in Case 2. A better approach is to look at the ratio between the observed and the expected counts as shown below: Risk_score Deaths Expected.Deaths OEratio 0 0 0.058 0.000 1 0 0.503 0.000 2 1 1.496 0.668 3 1 2.102 0.476 4 4 4.841 0.826 5 11 10.373 1.060 6 19 13.273 1.431 7 19 13.202 1.439 8 8 10.164 0.787 9 12 11.421 1.051 10 6 8.128 0.738 11–14 7 7.018 0.997 No score 17 18.458 0.921 If the death rates in this hospital system is about the same as the national rate, then we would expect these ratios to be close to 1 and hence the average of these difference should be close to 1. For this data, the mean of these ratios is 0.8 (0.7997 to be exact). Since this value is less than one, it is good news for the hospital system. It implies that they are doing better compared to the nation. But we need to know the uncertainty (standard error) of this estimate. Calculating the exact standard error of this ratio is quite difficult. However, we can use the bootstrap to come up with a reasonable estimate of the standard error. As before, we’ll resample the 13 ratios with replacement and build the bootstrap sampling distribution. Then we measure the spread (standard deviation) of this bootstrap distribution to find the standard error of the ratio. The spread of the bootstrap sampling distribution (standard deviation) is 0.1189. Combining this with the mean ratio of 0.8, we can say that the death rate of this hospital system is 0.8 times the national rate, with an uncertainty of 0.12. This is encouraging news to the hospital system. In addition, you can see from the above plot, a large number of the bootstrap ratios are less than one which suggests that the TRUE death rate would probably be less than 1 as well. But we don’t quite know that. That is the kind of inference we would like to draw. We will get to that in the next section. But before that we need take stock of what we’ve learned so far. Brief Summary Example 1: Established that we can use the bootstrap method to find the standard error of a statistic. Example 2 and 3: Discussed how the bootstrap method help us in finding standard errors of complicated statistic, like the correlation coefficient and unfamiliar ratios of random variables. 4.2 Bootstrap Confidence Intervals As alluded at the end of section 4.1, we would like to make inferences about population parameters like the TRUE(population) mean or the TRUE correlation or TRUE death rate ratio. In classical statistics, we did this using confidence intervals. For example, we can use the random sample in example 1 (service time) to build a confidence interval for the TRUE average service time. In classical statistics, we do this using the following formula: \\[ \\bar x \\ \\pm \\ t_{score} \\ . SE(\\bar x) \\] \\[ \\bar x \\ \\pm \\ t_{score} \\ . \\frac{S}{\\sqrt n} \\] where \\(S\\) is the sample standard deviation. The final interval can be easily calculated using R as follows: t.test( x = mysample)$conf.int ## [1] 13.83355 30.26096 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Now let us see how to use the Bootstrap method to construct a confidence interval from the same sample. There are 3 main approaches, but we are going to learn only two of them. The other is beyond the scope of undergraduate work. Bootstrap SE Plug-in Method As the name implies, we simply use the bootstrap SE and plug that value in as an estimate for the actual Standard error of the statistic. For the above example, we can use the bootstrap estimate of SE, which was 3.95, and compute the confidence interval. This method works well if the bootstrap sampling distribution is roughly normal. For other cases, it will not produce good results. Therefore, this method is not very helpful with more complicated statistics. Bootstrap Percentile Method The second method is more flexible, in that, it does not require the bootstrap sampling distribution to be normal. If we are calculating a 95% confidence interval, we simply find the \\(2.5^{th}\\) and the \\(97.5^{th}\\) percentiles of the bootstrap sampling distribution and those are our lower and upper confidence limits. For example, for the service time data, the confidence bounds are shown in the figure below. The 95% bootstrap confidence interval for the TRUE average service time (\\(\\mu\\)) is [14.74, 30.17]. The corresponding 95% classical confidence interval is [13.83, 30.26]. As you can see, the two intervals do not differ much. Can you think what the reason might be? The real question that you already might be asking in your head is: “Why do we need these bootstrap confidence intervals when we already know how to find confidence intervals based on CLT?” As in the case of calculating standard errors, the real benefit of this method comes when we DO NOT have classical confidence intervals or when it is extremely difficult to find one. Consider the correlation example we looked at (Example 4.2). There is no easy way to find a confidence interval for the TRUE correlation (\\(\\rho\\)). But the bootstrap provides an easy alternative. The following plot shows the confidence bounds calculated from the correlation data. The 95% bootstrap confidence interval is [0.8 , 0.94]. As you can see in the above plot, the bootstrap sampling distribution is quite skewed. Yet we can use the bootstrap percentile method to find a 95% confidence interval for the unknown parameter. This is another benefit of bootstrap confidence intervals. They do not require that the sampling distribution of the sample statistic being normal. As you recall, normality of the sampling distribution is a crucial assumption in classical confidence intervals that are based on the central limit theorem. To wrap up this section, let us calculate a 95% bootstrap confidence interval for the death rate data. The following plot shows the confidence bounds. The 95% bootstrap confidence interval is for the TRUE observed/expected death ratio is [0.56 , 1.03]. This is very important information for the hospital system. If you recall, the mean sample ratio was 0.8, which suggested that this hospital system is doing “better” compared to the national rate. But the confidence interval gave us a better understanding of the situation. Based on this interval, you can conclude that the this hospital system is not that different from the nation in terms of heart disease deaths. The apparent 0.8 (‘better’ performance) is a result of chance. Can you think how I arrived at this conclusion? That would be part of your HW :) 4.3 Things to Ponder Can we trust bootstrap confidence intervals? The short answer is YES, we can trust them (otherwise, why use it, right?). The only caveat is that the original sample should be a REPRESENTATIVE sample of the population. Without representativeness, we can’t do anything bootstrap or classical. This is a very important point that we (statisticians) need to keep in our mind. No matter how fancy your models and tools are, if your data is ‘garbage’, you end up with ‘garbage’ answers. Are there instances where bootstrap confidence intervals fail to live up to the true capture rate? Yes. Just like with classical confidence intervals, the bootstrap confidence intervals may fail to achieve the advertised capture rate. It can happen due to many reasons. One of the main ones is that the original sample is of poor quality (biased or non-representative). There are ways to improve these intervals but those modifications are beyond the scope of this course. What other benefits (other than calculating standard errors of statistics and constructing a confidence intervals) do we get from the bootstrap method? The most important one is the bootstrap sampling distribution. We get a visual representation of the shape and center and variability of the statistic. So why is it useful? Recall that we have no way of knowing the TRUE sampling distribution, and the bootstrap sampling distribution provides a great alternative to visualize how the TRUE sampling distribution MIGHT look like. Here are few other things to keep in mind: Pros: It provides a way to get a sense about the sampling distribution of any statistic. It is a very flexible technique that can be used to construct confidence intervals for a variety of parameters. In some cases, like estimating a population correlation or some other complicated parameter, there are no easy classical methods to construct a confidence interval. The existing classical methods for such problems are based on bunch of assumptions and complicated statistical maneuvers. However, the bootstrap method provides a easy to use alternative to construct confidence intervals even for complicated parameters. In some cases, like estimating a population ratio (e.g. in the case of Heart disease data), we do not have any classical method to construct a confidence interval, so the bootstrap method is our ONLY hope. Cons: It requires more computational power than classical methods. In some cases we need to resample a LOT and it can take some time to produce the bootstrap sampling distribution of a complicated statistic (we haven’t done these in our class). It requires some coding skills to implement the bootstrap method. In our case we used the ‘mosaic’ package, which simplified lot of the work. In some cases, we have to develop our own code, which requires some coding skills. Theory behind the bootstrap method is complex. So it is hard to convince a reasonable person that this method actually works and not some voodoo science. :) "],
["permutation-randomization-tests.html", "5 Permutation (Randomization) Tests 5.1 Alternative to the Two Sample t-test 5.2 Application to Categorical Data 5.3 Application to Linear Regression 5.4 Things to ponder", " 5 Permutation (Randomization) Tests In Chapter 4, we looked at a new method (The Bootstrap) to find standard errors and confidence intervals. In this chapter we are going to learn a new method to perform hypothesis tests. These tests are called Permutation tests. Unlike classical hypothesis tests, these tests are NOT based on Central Limit Theorem. We will learn the basics of these tests and their limitations through 3 examples. Let’s dive in. 5.1 Alternative to the Two Sample t-test Example 5.1 A sample of 16 healthy women aged 18 - 40 were randomly assigned to drink 24 ounces of either diet cola or water. Their urine was collected for three hours after ingestion of the beverage and calcium excretion (in mg.) was measured . The researchers were investigating whether people who drink diet cola tend to loose more calcium out of their system, which would increase the amount of calcium in the urine for diet cola drinkers. Ref: Larson, Amin, Olsen, and Poth, Effect of Diet Cola on Urine Calcium Excretion, Endocrine Reviews, 31[3]: June 2010. Here are the summary statistics for the cola experiment. Cola Water Sample Mean 56.00 mg 49.125 mg n 8 8 Now let’s look at the following dialog: You: The difference in means = 6.875 mg, so it APPEARS that the people who drank cola tend to lose more calcium than the people who drank water. Cola lobbyist: This is total nonsense! This difference is just a pure coincidence. You: You mean all of the high calcium values just happened to be in the cola group just by chance? Cola lobbyist: Hell yeah! Look, you and I both know that cola is just like water, right? You: No!! I don’t think so . But I’m willing to give you the benefit of the doubt for now, until we analyze the data. Cola lobbyist: OK, here’s what I mean. Look at the Cola group and the water group values: Cola: 50,62,48,55,58,61,58,56 Water: 48,46,54,45,53,46,53,48 If, for example, you exchange the 62 (in cola group) with the 45 in the water group the difference in means will be cola_mean=49.125 water_mean=51.25 49.125 - 51.25 = -2.125 !!!! See, I told you, would you now say that water is bad! My point is: some individuals naturally have high calcium in their urine so if the cola group happens to get one such individual like in the original sample it can inflate the mean. You: Not so fast, you just picked the best scenario favorable to your side. Why don’t we look at other possibilities. Cola lobbyist: What do you mean? You: Here’s what I mean. IF cola is the same as water, then we can simply start with 16 people, pick 8 at random and label them as cola group and the rest as water group. After all, according to you, cola is just like water, right? So any of these values can be just as likely to be in the cola group as in the water group. We can repeat this many times and calculate the mean difference for each random draw and see what are the likely values IF “cola is same as water” This is the main idea of the permutation tests. Under our null hypothesis, we can permute the group labels. In other words, the groups (in this case, cola and water) is not influential to the final outcome (in this case, calcium level). Let us look at the behavior of the difference in sample means if we were to permute the group labels. The way we perform this is quite simple. Step 1: Permute the group labels. Step 2: Calculate the sample means for the new groups and take the difference. Call this \\(d\\). Step 3: Repeat steps 1 and 2, and record all the mean differences, \\(d_i\\)’s, for \\(i=1,\\ldots , N\\), where \\(N\\) is a very large number. Step 4: Plot the permutation distribution of \\(d\\). Then we can see, how many permutations (scenarios) had a mean difference of 6.875 or higher. The following code and plot shows the permutation distribution for the cola experiment. observed_difference &lt;- diffmean( Calcium ~ Drink , data = mydata) PD &lt;- do(1000)*diffmean( Calcium ~ shuffle(Drink) , data = mydata) Cola_plot &lt;- ggplot(data = PD) + aes(x = diffmean) + geom_histogram() + geom_vline(xintercept = observed_difference, col=&#39;red&#39;, linetype = &#39;dashed&#39;) + labs(title = &#39;Does Cola increase calcium levels in urine?&#39;, subtitle = &#39;Data source: Endocrine Reviews, 31(3), June 2010.&#39;, x = &#39;Difference in means (Water - Calcium)&#39;) Cola_plot As you can see the observed difference in means is -6.875 (not that the value is negative because of order of subtraction, which is Water group - Calcium group ). This difference is quite unlikely IF cola has nothing to do with calcium levels. We can calculate how unlikely is this be means of a p-value. This p-value can be calculated as follows: Step 1: Count the number of permutations that gave rise to mean difference of -6.785 or less Step2 : Divide this count by the number of simulated scenarios. For the simulation we did, we found 10 scenarios with a mean difference of -6.875 or less which translates to a p-value of \\(10/1000 = 0.01\\). Remember, when you do your own simulation, your p-values would be slightly different. We can compare this p-value to the classical two sample t-test p-value. t.test( Calcium ~ Drink, data = mydata) The p-value from the t-test is 0.007, which is close to the permutation test p-value of 0.01. However, the t-test relies on the fact that the data comes from approximately normal population or the sample sizes are large enough to rely on the Central Limit Theorem. In this case, however, we can not be confident on either of these assumptions given the very small sample size. Therefore, the permutation test is a excellent alternative to bypass these kinds of strong assumptions and yet produce useful results. The only assumption we need to perform the permutation test is the exchangability of group labels under the null hypothesis. Let us look at another example to solidify this idea. 5.2 Application to Categorical Data Example 5.2 This example is from a case in the U.S. District Court for the Southern District of New York (Waisome v. Port Authority of New York &amp; New Jersey). In 1991, Felix Waisome and a group of plaintiffs sued the port authority of New York claiming racial bias against Black officers in the promotion process, because, according to him, the port authority officials were passing black officers at a lower rate compared to the white officers. 508 White officers and 64 Black officers took an exam for promotion. 455 Whites officers and 50 Black officers passed the exam. The pass rate for whites (\\(\\hat p_{white} = 455/508 = 89.56\\%\\)) and the pass rate for blacks (\\(\\hat p_{black} = 50/64 = 78.13\\%\\)). On face value, it seems like the pass rate for whites is higher than that of blacks. As statisticians we know that this is not the real issue. The real question is: could this be due to chance? The standard method to answer this question is to use the two sample proportions tests. \\(H_0:\\) There is no difference in pass rates. \\(p_{white}\\) = \\(p_{black}\\) \\(H_1:\\) The pass rate for whites is higher than that of blacks: \\(p_{white}\\) &gt; \\(p_{black}\\) The p-value from the two sample proportions test can be calculated using the following R-command: prop.test( x = c(455,50), n = c(508,64), alternative = &#39;greater&#39;) The p-value from the above test is 0.006, which suggests that a difference of this magnitude (89.56% vs 78.13%) in pass rates could not have happened by chance IF the TRUE pass rates of whites and blacks were the same. This test relies on the Central Limit Theorem which require large sample sizes. For the black group the sample size is 64, which is not small but not that large either. Therefore, the p-value from the test may not be that accurate. Let us now use the permutation approach to find a p-value. The main issue in question is that the police department was bias in passing the candidates. In order to answer this, we will assume that they are not bias. Under this premise an officer passing the exam does not depend on his or her race. This is what allow us to permute the race labels among the candidates. That is, we shuffle the group label and recalculate the difference in pass rates. We repeat this for lots of shuffles and build the permutation distribution as follows: # Create the data frame using the counts from each group race_info &lt;- c(rep(&#39;W&#39;,508), rep(&#39;B&#39;,64)) exam_info &lt;- c(rep(1,455), rep(0,53), rep(1,50), rep(0,14)) exam_data &lt;- data.frame(Race=race_info, Pass=exam_info) observed_difference &lt;- 455/508 - 50/64 PD &lt;- do(10000)*diffprop(Pass ~ shuffle(Race), data = exam_data) PassFail_plot &lt;- ggplot(data = PD) + aes(x = diffprop) + geom_histogram(bins = 20) + geom_vline(xintercept = observed_difference, col=&#39;red&#39;, linetype = &#39;dashed&#39;) + labs(title = &#39;Is There Evidence of Racial Bias?&#39;, subtitle = &#39;Data source: Waisome v. Port Authority. U.S District Court (1991)&#39;, x = &#39;Difference in pass rates (white - black)&#39;) PassFail_plot As you can see in the above plot, IF the police department did not use race as a factor in passing the candidates, then it is unlikely that we observe difference of this magnitude (11.43% = 89.56% - 78.13%) in pass rates. To be precise, the observed p-value is .0086. This small p-value raises a flag about the unbiasedness of the testing procedure. Does it mean that the police department is culpable of racial bias? 5.3 Application to Linear Regression Example 5.3 The final example of this chapter illustrates the flexibility of permutation tests. In this example, we will see how to apply this technique to test hypotheses about linear models. In particular, we will learn how to test a hypothesis about the slope coefficient in a simple linear regression model. The dataset we use here contain information about pizzas. You can access the dataset using the following command: Pizza &lt;- read_csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/Pizza.csv&#39;) Suppose we are interested in how much calories a pizza has and its relationship to the amount of fat in the pizza. The following plot shows the this relationship: As you can see, the relationship is linear and quite strong. The slope of the fitted line is 6.52. How do we assess whether this value is statistically significant? The classical method is to look at the t-statistic corresponding to the slope coefficient. We can state describe the test and the test statistic as follows: \\(H_0\\) : Slope( \\(\\beta\\) ) = 0 \\(H_1\\) : \\(\\beta \\neq 0\\) Test Statistic : \\(\\frac{\\hat \\beta - 0}{SE(\\hat \\beta)}\\) You can easily calculate the p-value associated with this test using the following R-command: mymodel &lt;- lm(calories ~ fat , data = Pizza) summary(mymodel) The p-value you get for the slope test is essentially zero. What this means is, IF fat has nothing to do with the calories in a pizza, then it is highly unlikely that we observe a slope coefficient of 6.52 by chance alone. This seems logical given the strong correlation that we see in the scatter plot above. Now let us see how to use the permutation approach to calculate the p-value for the slope coefficient. First, we need to ask is there any group label that we can permute. Recall that in all previous examples in this chapter we worked with two groups. e.g. cola vs. water, whites vs. blacks. In this case, there is no such group variable. But we can still use the permutation method. The key is with the null hypothesis. According to the null hypothesis, we assume that there is no linear relationship between fat and calories. Therefore, any of the calorie values could have occurred with any of the fat levels. Let us look at the first five rows of the data. Fat Original Calorie Values Shuffled Calorie Values 15 364 332 11 334 307 12 332 364 14 341 334 9 307 341 The reason we can permute the calorie values is because the null hypothesis states that fat can calories are not correlated. Hence any of the calorie values is equally likely to have occurred with any of the fat levels. This is the key point that we exploit to build our permutation distribution of slope estimates. As shown above, we shuffle the calorie values and refit the linear model and calculate the slope estimate. We repeat this process a large number of times and for each shuffle, we estimate the slope parameter. The following code and the plot shows the output of this process. observed_slope &lt;- coef(lm(calories ~ fat, data = Pizza))[2] PD &lt;- do(10000)*lm(calories ~ shuffle(fat), data = Pizza) Slope_plot &lt;- ggplot(data = PD) + aes(x = fat) + geom_histogram() + geom_vline(xintercept = observed_slope, col=&#39;red&#39;, linetype = &#39;dashed&#39;) + labs(title = &#39;Permutation Plot of Slope values&#39;, subtitle = &#39;Data source: Watkins, et al. (2010)&#39;, x = &#39;Slope coefficient&#39;) Slope_plot As we expected, under the premise of no correlation with fat and calories (null hypothesis), we expect most of the slope values to be around zero and with some spread ranging from -2.5 t0 2.5. But the observed slope of 6.52 (red dashed line) is almost impossible to occur by chance alone if there is no correlation with fat and calories. Hence we reject our null hypothesis of slope = 0 and conclude that the TRUE slope is significantly different from zero. Summary Permutation tests are quite useful alternatives for classical tests when there are doubts about the assumptions behind those classical tests. We can use them in a variety of settings, two sample tests for means, two sample proportions, linear regression. The key for any permutation test is that the null hypothesis should indicate a situation where it allows us to shuffle (permute) the data. 5.4 Things to ponder Can we use permutation tests in any situation? Answer No. There are many cases where we can not use permutation tests. For example, if we have single random sample and say that we want to test the TRUE mean \\(\\mu\\) = 10. There is nothing to permute and hence we can’t use the permutation method. Are the permutation tests resistant to outliers? Answer No. The outliers may get shuffled around but theirs effects will still be there. Also, when we calculate the p-value using the observed value of the difference or slope which are based on the original data. As a result the effects of the outliers will not disappear. Therefore, as with any analysis, it is very important to deal with outliers appropriately. Reminds me of a quote from two of my heroes in statistics: “There is no substitute for facing the facts in detail.” - John Tukey and Fredrick Mosteller. "],
["p-values-jelly-beans-and-mosquitoes.html", "6 P-values, Jelly-Beans, and Mosquitoes 6.1 What is a p-value? 6.2 P-values and Large Samples. 6.3 Slicing and Dicing the Data Summary", " 6 P-values, Jelly-Beans, and Mosquitoes The title of this chapter is rather strange. If you keep reading, you’ll see why I included the terms “Jelly Beans” and “Mosquitoes”. The main goal of this chapter is to better understand the concept of the p-value, its limitations and misinterpretations. Let’s dive in. 6.1 What is a p-value? The short answer is that it is a conditional probability. Then you might ask: “conditional on what?”. It is conditional on the null hypothesis. For example, suppose we are testing whether average heart rate of males is higher than that of females. \\(H_0:\\ \\mu_{male} = \\mu_{female}\\) \\(H_1:\\ \\mu_{male} &gt; \\mu_{female}\\) We collect data from two independent random samples. Say we have a sample mean difference of \\(\\bar x_{male} - \\bar x_{female} = 13\\) bpm (beats per minute). As good statisticians, first we ask the question: “Is this difference real or is it a result of chance?”. So we perform a two sample t test to answer this question. Suppose we got a really small p-value of .000001. How do we interpret this? Recall that the p-value is the probability of observing a sample mean difference of 13 (or more extreme) given the null hypothesis. That is: \\[ p-value = P[(\\bar X_{M} - \\bar X_{F} ) \\geq 13 \\ | \\ \\mu_{M} = \\mu_{F}] \\] In simple terms, all this is saying is, how likely it is to observe a sample mean difference of 13 (or more) IF males and females have similar heart rates. This is where the troubles start. Most people forget the last part of that statement. That is, they forget (or not aware of) the conditional nature of the p-value. As a result p-values get mischaracterized. Here are some examples: The p-value is the probability that the null hypothesis is true. 1 - (p-value) is the probability that the alternative hypothesis is true. If you carefully thought about the p-value, you’ll should see that neither of these statements is true. As mentioned earlier, this is a direct result of ignoring the conditional aspect of the p-value. Another issue that you’ll explore in your readings will be the “prosecutor’s fallacy”. This is also a quite common mischaracterization of p-values. Let’s take the same example above. The p-value was 0.000001 (1 in a million). Correct If males and females have the same heart rates, then a sample difference of 13 is almost impossible to occur (1 in a million). Incorrect With such a big sample difference of 13 beats per minute, the chance is very small (1 in a million) that males and females have the same heart rates. Do you see the slippery nature of p-values? Both statements look correct, but only one of them is actually correct. The next section discusses another strange aspect of the p-values. 6.2 P-values and Large Samples. As statisticians, we love large samples. The more information we have the better. However, we need be careful with the p-values that we get when we are dealing with very large samples. This goes at the heart of data analysis. That is, we should pay a LOT of attention to context when dealing with numbers. After all, data is simply numbers with a context. Let’s look at the following example: Example 6.1 Suppose you have two independent sample from two population and you wan to test the following hypothesis: \\(H_0:\\ \\mu_{1} = \\mu_{2}\\) \\(H_1:\\ \\mu_{1} &gt; \\mu_{2}\\) Before we jump in, let’s look at some descriptive statistics and plots about the two samples. The dataset can be accessed using the following link: mydata &lt;- read_csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/very_large_sample.csv&#39;) The plots below clearly shows that the two samples are almost identical both in shape and spread. Both samples have almost the same mean and median. Median Mean SD n group1 10.0197 10.0248 0.994 25000 group2 10.0025 10.0039 0.994 25000 The other important fact to observe is that the sample sizes are very large (25000 per group!). Now if we conduct a two sample t-test for mean difference, we get the p-value of 0.009. Normally, one would say: “there is statistically significant difference between the two group means”. Although, I would not recommend this type of language, I’m sure you may have seen statements like this in media and even in scientific publications. Along with this kind of language, comes the misunderstanding that the difference between the groups is important or practically meaningful. This is where the trouble is. Some people associate “statistical significance” with “practical significance”. What we ought to do is to estimate the magnitude of the TRUE difference between the groups and interpret that in relation to the context. A 95% confidence interval will give us a better idea of the TRUE mean difference, which in this case is [0.003, 0.038]. These values need to be put in context. For example, if the goal of the study is to find out whether the average heart rates of males and females are different, then a difference of 0.038 (upper confidence limit) beats per minutes is not practically significant although it is “statistically significant”. The main takeaway of this example is that when you have really large samples, tiny differences become statistically significant. So, if we are not careful in interpreting the results in the right way, we’ll end up with silly statements like “males and females have a statistically significant 0.038 bpm difference in heart rates!”. To better understand why tiny differences become statistically significant with large samples, let us look the form of the test statistic carefully. The general form of most test statistic (like the t-statistic) is: \\[\\frac{Estimate - Parameter }{Standard\\ Error}\\] For example, the t-statistic has the following form: \\[T = \\frac{\\bar X - \\mu }{\\frac{s}{\\sqrt n}}\\] where \\(\\mu\\) is the population mean and \\(s\\) is the sample standard deviation. What will happen to the T statistic if you have a large sample? How does that affect the p-value of the test, would the p-value increase or decrease? With some thought, and some rearranging of terms in the expression above, you’ll see the connection between the sample size \\(n\\) and the t-value and how it drives the corresponding p-value. 6.3 Slicing and Dicing the Data The next point that we want to look at is not a problem with p-values. It is a problem with us (the practitioners of statistics)! In practice, when we are working with datasets with may variables, it is quite tempting to perform lots of analyses hoping to find something interesting. For example, consider the heart rate example we discussed above. Your main goal was to see whether males and females have different average heart rates. Suppose you collected data from many individuals and in addition to their heart rates and gender you’ve collected other information like, race, hair color, eye color, income level (high, medium, low), education level, etc. You:: Wait, why are you collecting all these other data?. Statistician: Well…, they are here for the test and we just wanted to get some background information. After all, it does not cost us or them anything to have this extra information. It is better to have more than less, right? You: I guess… This is where the trouble starts. Suppose you conduct the test and you did not find any difference between the two groups (males vs. females). If you are gung ho about your theory(hypothesis) that there exist some difference in heart rates between males and females, then you will be quite disappointed to see that the data says something contrary to your hypothesis. Rather than accept the this reality, you go on quest to “prove” it somehow. That is, you start torturing the data until it confess. What do I mean by this? This is where the extra variables you collected come in to play. For example, you can test: African American males vs African American females OR African American males vs White females OR . . . White males with dark hair and blue eyes with low education and in low income level vs. White females in with blond hair and green eyes with high education and in high income level. The choices are endless (and stupid!). This kind of subgroup testing is what I call slicing and dicing the data. When you perform a lot of tests like this, the likelihood of you finding a statistically significant p-value is very high. In fact, you can show (we’ll do it in class) that this probability grows exponentially with the number of different subgroup tests you perform. So you are bound to find “something” even if there is nothing there. The following cartoon illustrates this phenomena quite beautifully. The jelly bean story: https://xkcd.com/882/ Another funny one: https://xkcd.com/1478/ Now you might say, “Well, this is a cooked-up example. No one in their right mind do such egregious things”. You are absolutely right. But the reality is entirely different from what we’d like to believe. The following article is a powerful example of this unfortunate practice. A recent example: Cornell scientist turned shoddy data into viral studies about what we eat: https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking A Slightly Different Example To drive this point home about p-values are good but humans are not, consider the following regression model and its p-value for the slope coefficient. Can you think what is going on in this case? The data: mydata &lt;- read_csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/FlightDelays2017.csv&#39;) Here is the plot: Delay Predictors Estimates CI p (Intercept) 9.45 7.13 – 11.77 &lt;0.001 FlightNo 0.00 0.00 – 0.01 0.020 Observations 4029 R2 / R2 adjusted 0.001 / 0.001 Can you find what is wrong with the above analysis?&quot; P-values in the Courtroom The Supreme Court of the United States in March 2011 decided the case Matrixx Inc vs. Siracusano. The company Matrixx Inc. is the manufacturer of the cold medicine Zicam. They publicly dismissed reports linking Zicam and anosmia (loss of the sense of smell) when it had evidence of a biological link between ingredients of Zicam and anosmia. They used the term “statistically significant” to hide their misdeeds. They told a lower court that they did not disclose the adverse effects because the differences between Zicam users and others were not statistically significant. But the Supreme Court disagrees. In delivering the opinion for a unanimous Court, justice Sonia Sotomayor said: &quot;A lack of statistically significant data does not mean that medical experts have no reliable basis for inferring a causal link between a drug and adverse events. The FDA similarly does not limit the evidence it considers for purposes of assessing causation and taking regulatory action to statistically significant data. In assessing the safety risk posed by a product, the FDA considers factors such as “strength of the association,” “temporal relationship of product use and the event,” “consistency of findings across available data sources,” “evidence of a dose-response for the effect,” “biologic plausibility,”….. Assessing the materiality of adverse event reports requires consideration of the source, content, and context of the reports. This is not to say that statistical significance (or the lack thereof) is irrelevant–only that it is not dispositive of every case.&quot; More information: https://www.oyez.org/cases/2010/09-1156 Summary When dealing with really large samples, we need to be extra careful with the p-values. Why? Because, tiny differences become statistically significant and as a result we may be tempted to believe that there is something “important” with the data. Always pay attention to context and think about practical significance and don’t fall in love with statistical significance. The more tests you perform, you are bound to find “something”. Then people have the tendency to come up with a “cute story” to justify the results. As you can see, the issue is not with p-values, it is with us. The practitioners of statistics (us) are responsible for this mess. I would like to close this chapter with the following remarkable quote. It captures the essence of this chapter. “p-values are like mosquitoes. They have an evolutionary niche somewhere and no amount of scratching, swatting, and spraying will dislodge them.” – Campbell, J. P. (Editor): Journal of Applied Psychology, 67(6), 691–700. (1982) "]
]
